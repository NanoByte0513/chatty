namespace chatty_fbs;

enum DType:byte {
    NONE = 0,
    INT8 = 1,
    UINT8 = 2,
    INT16 = 3,
    UINT16 = 4,
    INT32 = 5,
    UINT32 = 6,
    INT64 = 7,
    UINT64 = 8,
    FLOAT32 = 9,
    FLOAT16 = 10,
    FLOAT8 = 11,
    BFLOAT16 = 12,
    INT4 = 13,
}

enum ActivationBits:byte {
    INT4=0, INT8=1, INT16=2, FP16=3,
}

enum ActLayer:byte {
    NONE = 0,
    RELU = 1,
    GELU = 2,
    SILU = 3,
    SWISH = 4,
    SIGMOID = 5,
    TANH = 6,
    SOFTMAX = 7,
}

table ScaleInfo {
    shape: [int] (required);
    dtype: DType;
    offset: int64;
    data_size: int64;
    zero_point: int;
}

table Tensor {
    shape: [int] (required);
    dtype: DType;
    offset: int64;
    data_size: int64;
    scale: ScaleInfo;
}

table Norm {
    type: string (required);
    weight: Tensor (required);
    bias: Tensor;
    epsilon: float;
    scale_x: ScaleInfo;
    scale_o: ScaleInfo;
}

table LinearLayer {
    weight: Tensor (required);
    bias: Tensor;
    act_bits: ActivationBits;
    scale_x: ScaleInfo;
    scale_o: ScaleInfo;
}

table AttentionLayer {
    k_proj: LinearLayer (required);
    v_proj: LinearLayer (required);
    q_proj: LinearLayer (required);
    o_proj: LinearLayer (required);
    q_norm: Norm;
    k_norm: Norm;
    norm: Norm(required);
}

table FFNLayer {
    up_proj: LinearLayer (required);
    down_proj: LinearLayer (required);
    gate_proj: LinearLayer (required);
    norm: Norm (required);
    act_layer: ActLayer;
}

table TransformerLayer {
    layer_idx: int;
    attn_layer: AttentionLayer (required);
    ffn_layer: FFNLayer (required);
}

table Model {
    name: string (required);
    model_type: string (required);
    tokenizer: [byte] (required);
    input_embed: LinearLayer (required);
    output_norm: Norm (required);
    output_embed: LinearLayer;
    layers: [TransformerLayer] (required);
    // Attention params
    head_dim: int;
    kv_num_heads: int;
    q_num_heads: int;
}

root_type Model;