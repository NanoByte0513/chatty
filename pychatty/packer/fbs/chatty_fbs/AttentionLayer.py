# automatically generated by the FlatBuffers compiler, do not modify

# namespace: chatty_fbs

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class AttentionLayer(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = AttentionLayer()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsAttentionLayer(cls, buf, offset=0):
        """This method is deprecated. Please switch to GetRootAs."""
        return cls.GetRootAs(buf, offset)
    # AttentionLayer
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # AttentionLayer
    def KProj(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from chatty_fbs.LinearLayer import LinearLayer
            obj = LinearLayer()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # AttentionLayer
    def VProj(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from chatty_fbs.LinearLayer import LinearLayer
            obj = LinearLayer()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # AttentionLayer
    def QProj(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from chatty_fbs.LinearLayer import LinearLayer
            obj = LinearLayer()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # AttentionLayer
    def OProj(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from chatty_fbs.LinearLayer import LinearLayer
            obj = LinearLayer()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # AttentionLayer
    def Norm(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(12))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from chatty_fbs.Norm import Norm
            obj = Norm()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

def AttentionLayerStart(builder):
    builder.StartObject(5)

def Start(builder):
    AttentionLayerStart(builder)

def AttentionLayerAddKProj(builder, kProj):
    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(kProj), 0)

def AddKProj(builder, kProj):
    AttentionLayerAddKProj(builder, kProj)

def AttentionLayerAddVProj(builder, vProj):
    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(vProj), 0)

def AddVProj(builder, vProj):
    AttentionLayerAddVProj(builder, vProj)

def AttentionLayerAddQProj(builder, qProj):
    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(qProj), 0)

def AddQProj(builder, qProj):
    AttentionLayerAddQProj(builder, qProj)

def AttentionLayerAddOProj(builder, oProj):
    builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(oProj), 0)

def AddOProj(builder, oProj):
    AttentionLayerAddOProj(builder, oProj)

def AttentionLayerAddNorm(builder, norm):
    builder.PrependUOffsetTRelativeSlot(4, flatbuffers.number_types.UOffsetTFlags.py_type(norm), 0)

def AddNorm(builder, norm):
    AttentionLayerAddNorm(builder, norm)

def AttentionLayerEnd(builder):
    return builder.EndObject()

def End(builder):
    return AttentionLayerEnd(builder)

import chatty_fbs.LinearLayer
import chatty_fbs.Norm
try:
    from typing import Optional
except:
    pass

class AttentionLayerT(object):

    # AttentionLayerT
    def __init__(self):
        self.kProj = None  # type: Optional[chatty_fbs.LinearLayer.LinearLayerT]
        self.vProj = None  # type: Optional[chatty_fbs.LinearLayer.LinearLayerT]
        self.qProj = None  # type: Optional[chatty_fbs.LinearLayer.LinearLayerT]
        self.oProj = None  # type: Optional[chatty_fbs.LinearLayer.LinearLayerT]
        self.norm = None  # type: Optional[chatty_fbs.Norm.NormT]

    @classmethod
    def InitFromBuf(cls, buf, pos):
        attentionLayer = AttentionLayer()
        attentionLayer.Init(buf, pos)
        return cls.InitFromObj(attentionLayer)

    @classmethod
    def InitFromPackedBuf(cls, buf, pos=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, pos)
        return cls.InitFromBuf(buf, pos+n)

    @classmethod
    def InitFromObj(cls, attentionLayer):
        x = AttentionLayerT()
        x._UnPack(attentionLayer)
        return x

    # AttentionLayerT
    def _UnPack(self, attentionLayer):
        if attentionLayer is None:
            return
        if attentionLayer.KProj() is not None:
            self.kProj = chatty_fbs.LinearLayer.LinearLayerT.InitFromObj(attentionLayer.KProj())
        if attentionLayer.VProj() is not None:
            self.vProj = chatty_fbs.LinearLayer.LinearLayerT.InitFromObj(attentionLayer.VProj())
        if attentionLayer.QProj() is not None:
            self.qProj = chatty_fbs.LinearLayer.LinearLayerT.InitFromObj(attentionLayer.QProj())
        if attentionLayer.OProj() is not None:
            self.oProj = chatty_fbs.LinearLayer.LinearLayerT.InitFromObj(attentionLayer.OProj())
        if attentionLayer.Norm() is not None:
            self.norm = chatty_fbs.Norm.NormT.InitFromObj(attentionLayer.Norm())

    # AttentionLayerT
    def Pack(self, builder):
        if self.kProj is not None:
            kProj = self.kProj.Pack(builder)
        if self.vProj is not None:
            vProj = self.vProj.Pack(builder)
        if self.qProj is not None:
            qProj = self.qProj.Pack(builder)
        if self.oProj is not None:
            oProj = self.oProj.Pack(builder)
        if self.norm is not None:
            norm = self.norm.Pack(builder)
        AttentionLayerStart(builder)
        if self.kProj is not None:
            AttentionLayerAddKProj(builder, kProj)
        if self.vProj is not None:
            AttentionLayerAddVProj(builder, vProj)
        if self.qProj is not None:
            AttentionLayerAddQProj(builder, qProj)
        if self.oProj is not None:
            AttentionLayerAddOProj(builder, oProj)
        if self.norm is not None:
            AttentionLayerAddNorm(builder, norm)
        attentionLayer = AttentionLayerEnd(builder)
        return attentionLayer
